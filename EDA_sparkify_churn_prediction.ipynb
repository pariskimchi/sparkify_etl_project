{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9601ea87",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaa90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90129c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go \n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, concat, countDistinct,\\\n",
    "    col, datediff, date_format, desc, \\\n",
    "    format_number, isnan, lag, lit, udf, split\n",
    "\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer, VectorAssembler \n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "\n",
    "from pyspark.sql.functions import split \n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier \n",
    "from pyspark.ml.evaluation  import MulticlassClassificationEvaluator \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify_churn1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d3a94",
   "metadata": {},
   "source": [
    "#### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dataset\n",
    "path = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e121b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ad4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2081c",
   "metadata": {},
   "source": [
    "#### Column Split: Categorical  vs Numerical\n",
    "    => 데이터프레임의 각 컬럼을 분류해준다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "\n",
    "for i in range(len(df.dtypes)):\n",
    "    if df.dtypes[i][1] == 'string':\n",
    "        categorical_cols.append(df.dtypes[i][0])\n",
    "        \n",
    "    else:\n",
    "        numerical_cols.append(df.dtypes[i][0])\n",
    "\n",
    "print(\"Categorical columns:{}\".format(categorical_cols))\n",
    "print(\"Numerical columns : {}\".format(numerical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933796ab",
   "metadata": {},
   "source": [
    "### Categorical Columns\n",
    "- auth\n",
    "- gender \n",
    "- level \n",
    "- location \n",
    "- method \n",
    "-page\n",
    "-userAgent \n",
    "-userId\n",
    "\n",
    "=> 각 컬럼에 데이터를 이해하기 위해서 고유의 데이터들을 확인해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth \n",
    "df.select(\"auth\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender \n",
    "df.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ef8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level \n",
    "df.select(\"level\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location \n",
    "df.select(\"location\").distinct().show(), df.select(\"location\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method \n",
    "df.select(\"method\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e695017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page \n",
    "df.select(\"page\").distinct().show(), df.select(\"page\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userAgent \n",
    "df.select(\"userAgent\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c69108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId \n",
    "df.select(\"userId\").distinct().show(), df.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a57179",
   "metadata": {},
   "source": [
    "##### 컬럼 탐색 결과:\n",
    "- Auth: Authorization 과 관련된 데이터로 로그인, 로그아웃, 서비스탈퇴 등의 데이터\n",
    "- Page: 유저의 행동과 관련된 데이터로 로그인,NextSong(다음 노래 듣기),Submit upgrade, downgrade 등의 중요도가 높은 컬럼으로 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8898f3d",
   "metadata": {},
   "source": [
    "#### Page 컬럼 데이터 비율 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350713f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_list = df.select(\"page\").distinct().toPandas().values.tolist()\n",
    "\n",
    "# ratio check \n",
    "for i in range(len(page_list)):\n",
    "    page_name = page_list[i].__getitem__(0)\n",
    "    page_count = df.where(df['page']==page_name).count()/df.count()*100\n",
    "    \n",
    "    print(\"column {}\".format(page_name), page_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde248e",
   "metadata": {},
   "source": [
    "#### Ratio\n",
    "- nextsong:79%\n",
    "- Home: 5%\n",
    "- Thumbs up: 4%\n",
    "- Login, logout:1%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a28bd",
   "metadata": {},
   "source": [
    "#### 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58d3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# null value check on each columns \n",
    "for col in df.columns:\n",
    "    null_count = df.filter(\n",
    "        (df[col] == \"\") | (df[col].isNull()) | isnan(df[col])).count()\n",
    "    print('Column :{}'.format(col), null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a95fd",
   "metadata": {},
   "source": [
    "#### Page 결측치 체킹 결과:\n",
    "- artist,length, song :58392\n",
    "- firstName, gender, lastName, location, registration, userAgent, userId : 8346\n",
    "\n",
    "page 컬럼에는 Nextsong 과 같은 노래만 듣는 행동만 담긴게 아니라, Thumb up 혹은 submit upgrade 등의 여러가지 행동이 담겨 있으므로 artist, length, 결측치가 생길수 밖에 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da034237",
   "metadata": {},
   "source": [
    "그러므로 일단 userId가 없는 데이터 제거하고, sessionId 가 존재하는것만 살려본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values \n",
    "def drop_na(df):\n",
    "    \"\"\"\n",
    "        drop null value on userId, sessionId\n",
    "    \"\"\"\n",
    "    \n",
    "    # define filter condition \n",
    "    filter_user_id = (df['userId'] != \"\") & (df['userId'].isNotNull())& (~isnan(df['userId']))\n",
    "    filter_session_id = (df['sessionId'].isNotNull()) & (~isnan(df['sessionId']))\n",
    "    \n",
    "    df_clean = df.filter(filter_user_id).filter(filter_session_id)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply drop_na function \n",
    "df_clean = drop_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ecfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## null check again \n",
    "\n",
    "for col in df_clean.columns:\n",
    "    null_count = df_clean.filter(\n",
    "        (df_clean[col] == \"\") | (df_clean[col].isNull()) | isnan(df_clean[col])).count()\n",
    "    print('Column :{}'.format(col), null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eed66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count \n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f40e1",
   "metadata": {},
   "source": [
    "## 연습용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080db77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a0c6e2",
   "metadata": {},
   "source": [
    "##### Data preprocessing \n",
    "데이터 프레임을 전처리 하여 원하는 데이터 프레임으로 만들어준다\n",
    "- ts 컬럼을 통해 date, year, month, day, hour, dayofweek, weekofyear을 생성한다\n",
    "- location 컬럼에서 state를 추출해낸다\n",
    "- page 컬럼에서 Cancellation Confirmation, Submit downgrade 를 통해 \n",
    "    유저별로 canceled , downgraded 를 확인하는 컬럼, 그리고 이를 확인하는 churn_service, churn_paid 컬럼, 이탈할 가능성의 단계를 표시하는 phase_cancel, phase_downgrade  컬럼을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78544b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ts to timestamp, an dcreate date, year, month, day, hour, weekday, weekofyear\n",
    "def convert_ts(df):\n",
    "    \"\"\"\n",
    "        Convert timestamp column into serveral time columns \n",
    "    \"\"\"\n",
    "    ts = (F.col('ts')/1000).cast('timestamp')\n",
    "    \n",
    "    df_clean = df.withColumn('date',date_format(ts,format='yyyy-MM-dd'))\\\n",
    "        .withColumn('date',F.to_date(F.col('date'),'yyyy-MM-dd'))\\\n",
    "        .withColumn('year',F.year(F.col('date')))\\\n",
    "        .withColumn('month',F.month(F.col('date')))\\\n",
    "        .withColumn('day',F.dayofmonth(F.col('date')))\\\n",
    "        .withColumn('hour', F.hour(ts))\\\n",
    "        .withColumn('dayofweek',F.dayofweek(F.col('date')))\\\n",
    "        .withColumn('weekofyear',F.weekofyear(F.col('date')))\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ts to timestamp, an dcreate date, year, month, day, hour, weekday, weekofyear\n",
    "def convert_registration(df):\n",
    "    \"\"\"\n",
    "        Convert registration column into serveral time columns \n",
    "    \"\"\"\n",
    "    regi_ts = (F.col('registration')/1000).cast('timestamp')\n",
    "\n",
    "    df_regi = df.withColumn('regi_date',F.date_format(regi_ts,format='yyyy-MM-dd'))\\\n",
    "        .withColumn('regi_date',F.to_date(F.col('regi_date'),'yyyy-MM-dd'))\\\n",
    "            .withColumn('regi_year',F.year(F.col('regi_date')))\\\n",
    "            .withColumn('regi_month',F.month(F.col('regi_date')))\\\n",
    "            .withColumn('regi_day',F.dayofmonth(F.col('regi_date')))\\\n",
    "            .withColumn('regi_hour', F.hour(regi_ts))\\\n",
    "            .withColumn('regi_dayofweek',F.dayofweek(F.col('regi_date')))\\\n",
    "            .withColumn('regi_weekofyear',F.weekofyear(F.col('regi_date')))\n",
    "    \n",
    "    return df_regi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract location_state\n",
    "def extract_location_state(df):\n",
    "    \"\"\"\n",
    "        splits the location column to extract state\n",
    "    \"\"\"\n",
    "    df_extract =  df.withColumn('state',F.split(F.col('location'),\", \").getItem(1))\n",
    "    \n",
    "    return df_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_churn_label(df):\n",
    "    \"\"\"\n",
    "        Add churn_label, upgrade, downgrade, cancelled\n",
    "    \"\"\"\n",
    "    # udf to flag churn_service, churn_paid \n",
    "    add_cancelled = udf(lambda x:1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    add_downgraded = udf(lambda x:1 if x== \"Submit Downgrade\" else 0, IntegerType())\n",
    "    add_upgraded = udf(lambda x:1 if x==\"Submit Upgrade\" else 0, IntegerType())\n",
    "    \n",
    "    # apply udf and create flag column \n",
    "    df_flag = df.withColumn('cancelled',add_cancelled('page'))\\\n",
    "    .withColumn('downgraded',add_downgraded('page'))\\\n",
    "    .withColumn('upgraded',add_upgraded('page'))\n",
    "    \n",
    "    #set windowval and create phase columns \n",
    "    windowval = Window.partitionBy(['userId','level']).orderBy(F.desc('ts')).rangeBetween(Window.unboundedPreceding,0)\n",
    "    df_phase = df_flag.withColumn('phase_downgrade',F.sum('downgraded').over(windowval))\\\n",
    "        .withColumn('phase_upgrade',F.sum('upgraded').over(windowval))\n",
    "    \n",
    "    # phase_cancel\n",
    "    window_cancel = Window.partitionBy(['userId']).orderBy(F.desc('ts')).rangeBetween(Window.unboundedPreceding,0)\n",
    "    df_phase = df_phase.withColumn('phase_cancel',F.sum('cancelled').over(window_cancel))\n",
    "    \n",
    "    # create total churn num\n",
    "    df_churn = df_phase.withColumn('churn_paid',F.sum('downgraded').over(Window.partitionBy('userId')))\\\n",
    "    .withColumn('churn_service',F.sum('cancelled').over(Window.partitionBy('userId')))\n",
    "    \n",
    "    return df_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad566668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepared df\n",
    "def cleaned_df(df):\n",
    "    \"\"\"\n",
    "        clean and format dataframe \n",
    "    \"\"\"\n",
    "    df_clean = drop_na(df)\n",
    "    df_clean = convert_ts(df_clean)\n",
    "    df_clean = convert_registration(df_clean)\n",
    "    df_clean = extract_location_state(df_clean)\n",
    "    df_clean = set_churn_label(df_clean)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d57f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_format(spark_session, data_path):\n",
    "    \"\"\"\n",
    "        Loads a dataset file from the spark session \n",
    "        correspond to each data format\n",
    "    \"\"\"\n",
    "    #csv, json,parquet\n",
    "#     .split(\"/\")[-1].\n",
    "    \n",
    "    data_format = data_path.split(\".\")[-1]\n",
    "    \n",
    "    if data_format == \"json\":\n",
    "        return spark_session.read.json(data_path)\n",
    "    elif data_format == \"csv\":\n",
    "        return spark_session.read.csv(data_path)\n",
    "    elif data_format == \"parquet\":\n",
    "        return spark_session.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57744c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = load_data_format(spark,'mini_sparkify_event_data.json')\n",
    "\n",
    "# # apply clean function \n",
    "# df_clean = cleaned_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b383ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean function \n",
    "df_clean = cleaned_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8989abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking \n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234dc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878b8ac",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis \n",
    "- 유저의 이탈율(churn rate)에 대해 분석해본다.\n",
    "- 유저 데이터를 통해 이탈율(churn rate)와 관련이 있는 데이터를 분석하고, 추출해본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052b6d3",
   "metadata": {},
   "source": [
    "##### Define Churn \n",
    "- 1. Churn from service \n",
    "    => page 컬럼에서 Cancellation confirmation 한것을 서비스에서 이탈했다고 정의한다.\n",
    "- 2. Churn from paid membership \n",
    "    => page 컬럼에서 Submit downgrade 한것을 서비스 유료 결제 멤버쉽에서 이탈했다고 정의한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42df591",
   "metadata": {},
   "source": [
    "##### Auth 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth checking \n",
    "df_clean.select('auth').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca67e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.select(['auth','page'])\\\n",
    "    .filter(df_clean['auth'] == 'Cancelled').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736cf81",
   "metadata": {},
   "source": [
    "page 컬럼의 Cancellation confirmation 하면\n",
    "그 이후로는 auth상태는 Cancelled 로 변환이 된다.\n",
    "즉 auth:Cancelled 를 최종적으로 churn 되었다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859444d9",
   "metadata": {},
   "source": [
    "#### 이탈한 유저 데이터 탐색하기\n",
    "- userId:32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628495cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user 32 \n",
    "\n",
    "df_clean.select(['userId','auth','level','page','year','month','day','hour'])\\\n",
    "    .filter(df_clean['userId'] == '32').orderBy('hour' ,ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a32981",
   "metadata": {},
   "source": [
    "#### userId:32 의 관찰 결과 \n",
    "- paid 유저가 Cancellation confirmation 하고 auth가 Cancelled 된것을 확인 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a6a5f",
   "metadata": {},
   "source": [
    "#### 유료 결제 해지를 한 유저의 데이터 탐색 \n",
    "- userId:11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75371f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user 11 \n",
    "# explore user 11 data \n",
    "# 2018 10 11 15\n",
    "df_clean.select(['userId','auth','level','ts','page','year','month','day','hour'])\\\n",
    "    .filter(df_clean['userId']=='11')\\\n",
    "    .filter(df_clean['month'] == 10)\\\n",
    "    .filter((df_clean['day'] >= 10) & (df_clean['day'] <=11))\\\n",
    "    .filter((df_clean['ts']>='1539239509000')&(df_clean['ts']<='1539240639000'))\\\n",
    "    .orderBy('ts', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae88de6",
   "metadata": {},
   "source": [
    "Submit downgrade 요청하면 , level이 free로 바뀌는걸 확인 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa9806",
   "metadata": {},
   "source": [
    "#### Customer segmentation\n",
    "고객들을 유형별로 나눠서 분류하기 위한 상태체크 데이터가 필요하다고 생각해서 \n",
    "1. Churn from service (플랫폼에서 벗어난 유저상태)\n",
    "    - cancelled (cancel confirmation 할 경우에 cancelled 체크 컬럼)\n",
    "    - churn_service (이미 플랫폼에서 벗어난 유저그룹 체크 컬럼)\n",
    "        :0일경우 벗어나지 않았고, 1일 경우 True\n",
    "    - phase_cancel(서비스에서 이탈할때까지의 구간동안에 일어난 이벤트 데이터 체크)\n",
    "        : 1일 경우에 이탈하기전까지의 일어난 데이터라고 보면된다\n",
    "\n",
    "2. Churn from paid (유료결제를 해지한 유저)\n",
    "    - downgraded (downgrade, submit Downgrade 적용된경우)\n",
    "        : 적용 되면 1\n",
    "    - churn_paid (한번이라도 downgrade, submit downgrade 가 적용되었을 경우에 churn_paid 컬럼을 통해 확인 가능)\n",
    "       : 유료 결제 해지했을 경우에 1, 2번 해지했을 경우에 2\n",
    "    - phase_downgrade(유료 결제에서 이탈하기까지의 구간동안에 일어난 이벤트 데이터 체크) \n",
    "        : 1일 경우에 유료결제에서 이탈하기 전까지의 일어난 데이터, 2일 경우에 최근 시간에서 역순으로 유료결제 해지하기전까지의 구간데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e888a4",
   "metadata": {},
   "source": [
    "#### 이탈 유저 데이터 탐색 \n",
    "- userId:18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957c52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['userId','level','auth','page','ts','cancelled','phase_cancel','churn_service','downgraded','phase_downgrade','churn_paid']\n",
    "event_cols = ['userId','level','auth','cancelled','phase_cancel','churn_service','downgraded','phase_downgrade','churn_paid']\n",
    "## check: user 18\n",
    "\n",
    "df_clean.filter(df_clean['userId']=='18')\\\n",
    "    .select(label_cols)\\\n",
    "    .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.filter(df_clean['userId']=='18')\\\n",
    "    .select(event_cols)\\\n",
    "    .distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194265d",
   "metadata": {},
   "source": [
    "###### 유저 18 결과\n",
    "- 유저18은 항상 유료결제\n",
    "- 유료결제로 이용하다가 마지막에 서비스에서 이탈한것을 확인 할수 있다 \n",
    "- 그러므로 churn_service = 1 로 유저 18의 모든 데이터가 이탈이용 구간으로 체킹 된것을 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425472d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd14a01c",
   "metadata": {},
   "source": [
    "##### 유료결제 이탈 유저 탐색\n",
    "- user 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_cols \n",
    "df_clean.filter(df_clean['userId']=='11')\\\n",
    "    .select(event_cols)\\\n",
    "    .distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdb9b0",
   "metadata": {},
   "source": [
    "- 위의 데이터에서 유저 11은 일단 유료결제 멤버쉽 해지를 1번 한적이 있는 유저로, churn_paid=1 로써 확인가능\n",
    "- 유료결제 해지까지 구간은 phase_downgrade=1 로써 구분 가능.\n",
    "- 하지만 유저 11은 무료에서 다시 유료결제 멤버쉽을 가입한것을 확인 할 수 있다.\n",
    "- 유저 11이 다시 paid 에서 한번더 이탈한다면 => churn_paid=2 가 될것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca332b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user 11\n",
    "\n",
    "df_clean.filter(df_clean['userId']=='11')\\\n",
    "    .select(label_cols)\\\n",
    "    .filter(df_clean['month'] == 10)\\\n",
    "    .filter((df_clean['day'] >= 10) & (df_clean['day'] <=11))\\\n",
    "    .filter((df_clean['ts']>='1539239509000')&(df_clean['ts']<='1539240639000'))\\\n",
    "    .orderBy('ts', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905445a",
   "metadata": {},
   "source": [
    "#### 결제 이탈 + 서비스 이탈 유저 탐색 \n",
    "- 결제에서 이탈하기도 하고 , 그리고 최종적으로 서비스에서도 이탈한 유저를 탐색해본다\n",
    "- userId:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_cols \n",
    "df_clean.filter(df_clean['userId']=='12')\\\n",
    "    .select(event_cols)\\\n",
    "    .orderBy('ts', ascending=False)\\\n",
    "    .distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fae00",
   "metadata": {},
   "source": [
    "- 위의 데이터를 보면 최종적으로 churn_service=1, churn_paid=1을 통해 결제와 서비스에서 이탈한것을 확인 가능\n",
    "- 1. 무료=> 유료 결제로 해지하기 까지의 phase_downgrade=1 컬럼 구간을 통해 구분 가능하고\n",
    "- 2. 다시 free=> paid로 결제\n",
    "- 3. 하지만 최종적으로는 cancelled=1 로 서비스에서 이탈하였으므로 churn_service=1로 최종적으로 구분가능\n",
    "- 최종적으로 모든 유저 12의 데이터는 서비스에서 이탈한 유저의 데이터로써 볼 수 있음과 동시에 결제에서 이탈했다고도 볼수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d79ff",
   "metadata": {},
   "source": [
    "##### 이탈하지 않은 유저 탐색 \n",
    "- 결제 및 서비스에서 이탈하지 않은 유저 데이터 탐색해본다\n",
    "- userId:104\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_churn_service = F.col('churn_service') !=0\n",
    "filter_churn_paid = F.col('churn_paid') !=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837178c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_cols \n",
    "df_clean.filter(df_clean['userId']=='104')\\\n",
    "    .select(event_cols)\\\n",
    "    .orderBy('ts', ascending=False)\\\n",
    "    .distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53810f4a",
   "metadata": {},
   "source": [
    "- 위의 유저 104데이터를 통해 free에서 paid로 유료결제 한 후로 쭉 이용하고 있는 유저로 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be7bfd",
   "metadata": {},
   "source": [
    "#### Churn user count analysis\n",
    "- 이탈 유저 수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## total number of users \n",
    "total_num_users = df_clean.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17736be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. number of churn_paid users \n",
    "# churn_paid =1 , userId \n",
    "num_churn_paid = df_clean.select('userId')\\\n",
    "    .filter(df_clean['churn_paid'] !=0)\\\n",
    "    .distinct().count()\n",
    "\n",
    "print('{} users Cancelled paid level from the service'.format(num_churn_paid))\n",
    "print('{:.2f}% of users are downgraded'.format(num_churn_paid/total_num_users*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. number of churn_service users \n",
    "num_churn_service = df_clean.select('userId')\\\n",
    "    .filter(df_clean['churn_service'] !=0)\\\n",
    "    .distinct().count()\n",
    "\n",
    "print('{} users unsubscribed from the service'.format(num_churn_service))\n",
    "print('{:.2f} of users are unsubscribed'.format(num_churn_service/total_num_users*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. number of churn_paid then, chrun_service either \n",
    "filter_churn_service = F.col('churn_service')!=0\n",
    "filter_churn_paid = F.col('churn_paid') !=0\n",
    "\n",
    "num_churn_paid_service = df_clean.select('userId')\\\n",
    "    .filter(filter_churn_service&filter_churn_paid).distinct().count()\n",
    "\n",
    "print('{} users downgraded and unsubscribed'.format(num_churn_paid_service))\n",
    "print('{:.2f}% of users are downgraded and unsubscribed'.format(num_churn_paid_service/total_num_users*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514936c6",
   "metadata": {},
   "source": [
    "#### Explore Data \n",
    "- Impact of gender\n",
    "- Impact of the subscription level(paid vs free)\n",
    "- Impact of location\n",
    "- Impact of avg number of repeat\n",
    "- Impact of the avg number of ads \n",
    "- Impact of the number of logins and time between two logins\n",
    "- Impact of login count \n",
    "- Impact of daily behavior\n",
    "- Impact of the listening time per session\n",
    "- Impact of the time of the activity (count of actions)\n",
    "    - week\n",
    "    - month\n",
    "- impact of the time between registration, upgrade and downgrade events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336234d",
   "metadata": {},
   "source": [
    "#### Impact of the gender\n",
    "- Cancellation analysis per gender \n",
    "    - churned_user vs active_user \n",
    "- Downgrade analysis \n",
    "    - churned_paid user vs active paid user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_number column = recent value\n",
    "windowval = Window.partitionBy('userId').orderBy(F.desc('ts'))\n",
    "df_clean = df_clean.withColumn('row_number',F.row_number().over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b97bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set segmetation data \n",
    "filter_churn_service = F.col('churn_service') !=0\n",
    "filter_churn_paid = F.col('churn_paid') !=0\n",
    "filter_no_churn = ~(filter_churn_service|filter_churn_paid)\n",
    "filter_churn_either = (filter_churn_service&filter_churn_paid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f24cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer segmentation\n",
    "gender_cols = ['userId','gender']\n",
    "\n",
    "gender_no_churn = df_clean.select(['userId','gender']).filter(filter_no_churn).distinct().toPandas()\n",
    "gender_churn_paid = df_clean.select(gender_cols)\\\n",
    "    .filter(filter_churn_paid).distinct().toPandas()\n",
    "gender_churn_service = df_clean.select(gender_cols)\\\n",
    "    .filter(filter_churn_service).distinct().toPandas()\n",
    "\n",
    "gender_active_paid = df_clean.filter(F.col('row_number')==1)\\\n",
    "    .filter(F.col('level')=='paid').filter(filter_no_churn)\\\n",
    "    .select('userId','gender').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique user df\n",
    "df_total_user = df_clean.select(['userId','gender']).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25991ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total male, female count\n",
    "num_total_male = df_total_user[df_total_user['gender']=='M'].gender.count()\n",
    "num_total_female = df_total_user[df_total_user['gender']=='F'].gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaef76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancellation analysis \n",
    "# male \n",
    "print('churnd user')\n",
    "print('male ratio:',gender_churn_service[gender_churn_service.gender=='M'].gender.count()/num_total_male)\n",
    "print('female ratio:',gender_churn_service[gender_churn_service.gender=='F'].gender.count()/num_total_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4634596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio_by_gender(df,title):\n",
    "    \n",
    "    df_total_user = df_clean.select(['userId','gender']).distinct().toPandas()\n",
    "    \n",
    "    num_total_male = df_total_user[df_total_user['gender']=='M'].gender.count()\n",
    "    num_total_female = df_total_user[df_total_user['gender']=='F'].gender.count()\n",
    "    \n",
    "    print(title)\n",
    "    print('male ratio:',df[df.gender=='M'].count()[0]/num_total_male)\n",
    "    print('female ratio:',df[df.gender=='F'].count()[0]/num_total_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e106759",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratio_by_gender(gender_no_churn,'active no churn user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratio_by_gender(gender_churn_paid,'churn paid user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16439e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratio_by_gender(gender_active_paid,'active paid user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=['Male','Female'],\n",
    "    y=[sum(gender_churn_service.gender == 'M'),sum(gender_churn_service.gender=='F')],\n",
    "    name='churned user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=['Male','Female'],\n",
    "    y=[sum(gender_no_churn.gender == 'M'),sum(gender_no_churn.gender=='F')],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=['Male','Female'],\n",
    "    y=[sum(gender_churn_paid.gender=='M'),sum(gender_churn_paid.gender=='F')],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    x=['Male','Female'],\n",
    "    y=[sum(gender_active_paid.gender=='M'), sum(gender_active_paid.gender=='F')],\n",
    "    name='active paid user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "\n",
    "fig.layout.update(height=500, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'Gender'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text = 'Count of users'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b4791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdcc57cc",
   "metadata": {},
   "source": [
    "- 위의 그래프로 보아서 성별의 영향이 큰지는 딱히 잘 모르겠지만 그래프를 통해서 \n",
    "- 남성이 서비스에서 이탈할 확률이 근소하게 더 높은것 같고 \n",
    "- 결제에서의 이탈 확률은 여성이 조금 더 높다는걸 확인할수 있다. \n",
    "- 하지만 유저 수가 225명으로 얼마 되지 않기 떄문에 큰 영향은 확신할수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aee82",
   "metadata": {},
   "source": [
    "#### Impact of level (free vs paid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_no_churn = df_clean.select(['userId','level'])\\\n",
    "    .filter(F.col('row_number')==1)\\\n",
    "    .filter(filter_no_churn).distinct().toPandas()\n",
    "\n",
    "level_churn_service= df_clean.select(['userId','level'])\\\n",
    "    .filter(F.col('row_number')==1)\\\n",
    "    .filter(filter_churn_service).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7565bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=['Paid', 'Free'],\n",
    "    y=[100*len(level_churn_service[level_churn_service['level']=='paid'])/level_churn_service.shape[0],\n",
    "      100*len(level_churn_service[level_churn_service['level']=='free'])/level_churn_service.shape[0]],\n",
    "    name='churned user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=['Paid', 'Free'],\n",
    "    y=[100* len(level_no_churn[level_no_churn['level']=='paid'])/level_no_churn.shape[0],\n",
    "      100* len(level_no_churn[level_no_churn['level']=='free'])/level_no_churn.shape[0]],\n",
    "    name='active user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "\n",
    "\n",
    "fig.layout.update(height=500, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'Level'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text = 'Count of users'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ba8ad",
   "metadata": {},
   "source": [
    "Level결과\n",
    "- 위의 결과로 서비스 이탈 유저는 paid 의 비율이 높은걸 확인 가능 \n",
    "- 하지만 현재 사용 유저중에서의 paid의 비율이 높은걸로도 확인가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f41c0b",
   "metadata": {},
   "source": [
    "#### Impact of the location of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.select(['location','state']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_cols = ['userId','state']\n",
    "\n",
    "location_no_churn = df_clean.select(['userId','state','level'])\\\n",
    "    .filter(filter_no_churn).distinct().toPandas()\n",
    "location_churn_paid = df_clean.select(location_cols)\\\n",
    "    .filter(filter_churn_paid).distinct().toPandas()\n",
    "location_churn_service = df_clean.select(location_cols)\\\n",
    "    .filter(filter_churn_service).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9661875",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df_clean.select('state').distinct().toPandas()['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85365",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=locations,\n",
    "    y=location_churn_service.groupby('state')['userId'].count(),\n",
    "    name='churned user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=locations,\n",
    "    y=location_no_churn.groupby('state')['userId'].count(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=locations,\n",
    "    y=location_churn_paid.groupby('state')['userId'].count(),\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    x=locations,\n",
    "    y=location_no_churn[location_no_churn['level']=='paid'].groupby('state')['userId'].count(),\n",
    "    name='active paid user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e712d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "\n",
    "fig.layout.update(height=500, width=1000, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'location'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text = 'Count of users'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3baf7",
   "metadata": {},
   "source": [
    "location 결과\n",
    "- 특정 지역에서 서비스 이탈이 많은걸 확인 할수 있지만, 유저가 그리 많지 않아서 큰 의미는 잘 모르겠다.\n",
    "- 하지만 유료결제 이탈에서의 LA 지역이 뚜렷하게 나타난다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319d7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7204ecb3",
   "metadata": {},
   "source": [
    "#### Impact of avg number of repeat\n",
    "- 같은노래의 반복 횟수를 분석해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.select(['userId']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_cols = ['userId','level','churn_service','churn_paid']\n",
    "\n",
    "df_count_repeat = df_clean.groupby(repeat_cols)\\\n",
    "    .agg(F.count('song').alias('total_num_song'),\\\n",
    "        F.countDistinct('song').alias('unique_num_song'),\\\n",
    "        F.countDistinct('artist').alias('distinct_artist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94996981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_repeat.orderBy('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ratio of repeat song, distinct artist \n",
    "\n",
    "df_count_repeat = df_count_repeat\\\n",
    "    .withColumn('repeat_ratio',F.round(100-(100 * df_count_repeat['unique_num_song']/df_count_repeat['total_num_song'])))\\\n",
    "    .withColumn('repeat_distinct_artist', F.round(100*df_count_repeat['distinct_artist']/df_count_repeat['total_num_song']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation\n",
    "repeat_cols = ['userId','level','repeat_ratio','repeat_distinct_artist','distinct_artist']\n",
    "\n",
    "count_repeat_no_churn = df_count_repeat.filter(filter_no_churn)\\\n",
    "    .select(repeat_cols).distinct().dropna().toPandas()\n",
    "\n",
    "count_repeat_churn_paid = df_count_repeat.filter(filter_churn_paid)\\\n",
    "    .select(repeat_cols).distinct().dropna().toPandas()\n",
    "count_repeat_churn_service = df_count_repeat.filter(filter_churn_service)\\\n",
    "    .select(repeat_cols).distinct().dropna().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active user ratio\n",
    "count_repeat_no_churn.repeat_ratio.mean(),count_repeat_no_churn.repeat_distinct_artist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn_service ratio\n",
    "count_repeat_churn_service.repeat_ratio.mean(), count_repeat_churn_service.repeat_distinct_artist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn paid\n",
    "count_repeat_churn_paid.repeat_ratio.mean(),count_repeat_churn_paid.repeat_distinct_artist.mean(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e53681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active paid user \n",
    "count_repeat_active_paid = count_repeat_no_churn[count_repeat_no_churn['level']=='paid']\n",
    "\n",
    "count_repeat_active_paid.repeat_ratio.mean(), count_repeat_active_paid.repeat_distinct_artist.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e09850",
   "metadata": {},
   "source": [
    "##### visualization : number of repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eae589",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=['repeat','distinct_artist'],\n",
    "    y=[# churn_service ratio\n",
    "count_repeat_churn_service.repeat_ratio.mean(), count_repeat_churn_service.repeat_distinct_artist.mean()],\n",
    "    name='churned user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=['repeat','distinct_artist'],\n",
    "    y=[count_repeat_no_churn.repeat_ratio.mean(),count_repeat_no_churn.repeat_distinct_artist.mean()],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=['repeat','distinct_artist'],\n",
    "    y=[count_repeat_churn_paid.repeat_ratio.mean(),count_repeat_churn_paid.repeat_distinct_artist.mean()],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    x=['repeat','distinct_artist'],\n",
    "    y=[count_repeat_active_paid.repeat_ratio.mean(), count_repeat_active_paid.repeat_distinct_artist.mean()],\n",
    "    name='active paid user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc989f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "\n",
    "fig.layout.update(height=500, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'repeat'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text = 'avg ratio of the user'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e09bf0",
   "metadata": {},
   "source": [
    "number of repeat 결과\n",
    "- 서비스나 유료결제에서의 이탈 유저의 평균 같은 노래 반복 비율은 상대적으로, 새로운 노래를 듣는 비율보다 낮으며 활성화된 이용 유저들보다 낮다.\n",
    "- 같은 노래를 듣는 반복 비율이 낮으면 => 서비스, 유료결제에서의 이탈에 영향을 준다고 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16164172",
   "metadata": {},
   "source": [
    "### Impact of the avg number of ads\n",
    "- 평균 광고 시청 회수 분석을 통해 이탈율에게 영향을 주는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ads = udf(lambda x: 1 if x == 'Roll Advert' else 0 , IntegerType())\n",
    "is_song = udf(lambda x:1 if x == 'NextSong' else 0, IntegerType())\n",
    "\n",
    "# apply udf to on page column \n",
    "\n",
    "df_count_ads = df_clean.withColumn('is_song', is_song(F.col('page')))\\\n",
    "    .withColumn('is_ads', is_ads(F.col('page')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6826d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_ads = df_count_ads.groupby(['userId','level','churn_service','churn_paid'])\\\n",
    "    .agg(F.sum('is_song').alias('num_song'),\\\n",
    "        F.sum('is_ads').alias('num_ads'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_ads = df_count_ads.withColumn('ratio_song_ads',F.round(100*df_count_ads['num_ads']/df_count_ads['num_song'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 422 count\n",
    "df_count_ads.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_cols = ['userId','ratio_song_ads','level']\n",
    "\n",
    "count_ads_no_churn = df_count_ads.filter(filter_no_churn)\\\n",
    "    .select(ads_cols).distinct().toPandas()\n",
    "count_ads_churn_paid = df_count_ads.filter(filter_churn_paid)\\\n",
    "    .select(ads_cols).distinct().toPandas()\n",
    "count_ads_churn_service = df_count_ads.filter(filter_churn_service)\\\n",
    "    .select(ads_cols).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service_paid vs active_paid\n",
    "# churn_Service_paid_user\n",
    "trace1 = go.Box(\n",
    "    x=count_ads_churn_service[count_ads_churn_service['level']=='paid']['ratio_song_ads'],\n",
    "    name='churned service paid user'\n",
    ")\n",
    "# active paid user\n",
    "trace2 = go.Box(\n",
    "    x=count_ads_no_churn[count_ads_no_churn['level']=='paid']['ratio_song_ads'],\n",
    "    name='active paid user'\n",
    ")\n",
    "\n",
    "# 2. churn_Service free user vs active free user\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=count_ads_churn_service[count_ads_churn_service['level']=='free']['ratio_song_ads'],\n",
    "    name='churn service free user'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=count_ads_no_churn[count_ads_no_churn['level']=='free']['ratio_song_ads'],\n",
    "    name='active free user'\n",
    ")\n",
    "\n",
    "### 3. downgrade: churn_paid user vs active paid user\n",
    "\n",
    "trace5 = go.Box(\n",
    "    x=count_ads_churn_paid[count_ads_churn_paid['level']=='paid']['ratio_song_ads'],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace6 = go.Box(\n",
    "    x=count_ads_no_churn[count_ads_no_churn['level']=='paid']['ratio_song_ads'],\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "fig.append_trace(trace5,3,1)\n",
    "fig.append_trace(trace6,3,1)\n",
    "\n",
    "\n",
    "fig.layout.update(height=900, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'ratio of ads listened : Active vs churned user'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text = 'ratio of ads'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45730fe",
   "metadata": {},
   "source": [
    "광고 횟수 분석 결과\n",
    "- 전체적으로 서비스나 유료 멤버쉽에서의 이탈한 유저들은 상대적으로 노래 대비 광고 시청 비율이 높았다.\n",
    "- free 유저가 paid 유저보다 평균적으로 약 10배정도 광고를 더 시청한다\n",
    "- 광고 시청 횟수 비율이 이탈율에 영향을 주는것 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf463d24",
   "metadata": {},
   "source": [
    "#### Impact of the number of logins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcb338",
   "metadata": {},
   "source": [
    "##### count login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6504e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login count df\n",
    "df_count_login = df_clean.groupby(['userId','level','churn_service','churn_paid'])\\\n",
    "    .agg(F.countDistinct('sessionId').alias('count_login'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking \n",
    "df_count_login.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_cols = ['userId','count_login','level']\n",
    "\n",
    "login_no_churn = df_count_login.filter(filter_no_churn)\\\n",
    "    .select(login_cols).toPandas()\n",
    "login_churn_paid = df_count_login.filter(filter_churn_paid)\\\n",
    "    .select(login_cols).toPandas()\n",
    "login_churn_service = df_count_login.filter(filter_churn_service)\\\n",
    "    .select(login_cols).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"active user\",login_no_churn.count_login.mean())\n",
    "print(\"churn service user\", login_churn_service.count_login.mean())\n",
    "print(\"active paid user\",login_no_churn[login_no_churn['level']=='paid'].count_login.mean())\n",
    "print(\"churn paid user\",login_churn_paid.count_login.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service_paid vs active_paid\n",
    "# churn_Service_paid_user\n",
    "trace1 = go.Box(\n",
    "    x=login_churn_service[login_churn_service['level']=='paid']['count_login'],\n",
    "    name='churned service paid user'\n",
    ")\n",
    "# active paid user\n",
    "trace2 = go.Box(\n",
    "    x=login_no_churn[login_no_churn['level']=='paid']['count_login'],\n",
    "    name='active paid user'\n",
    ")\n",
    "\n",
    "# 2. churn_Service free user vs active free user\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=login_churn_service[login_churn_service['level']=='free']['count_login'],\n",
    "    name='churn service free user'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=login_no_churn[login_no_churn['level']=='free']['count_login'],\n",
    "    name='active free user'\n",
    ")\n",
    "\n",
    "### 3. downgrade: churn_paid user vs active paid user\n",
    "\n",
    "trace5 = go.Box(\n",
    "    x=login_churn_paid[login_churn_paid['level']=='paid']['count_login'],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace6 = go.Box(\n",
    "    x=login_no_churn[login_no_churn['level']=='paid']['count_login'],\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "fig.append_trace(trace5,3,1)\n",
    "fig.append_trace(trace6,3,1)\n",
    "\n",
    "\n",
    "fig.layout.update(height=900, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'count of login : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0314748",
   "metadata": {},
   "source": [
    "#### avg time between two login(delta time)\n",
    "- 세션 사이의 평균 시간차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495476d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_user_level = df_clean.filter(F.col('row_number')==1)\\\n",
    "    .select(['userId','level']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by userd and sorting by descending date\n",
    "windowval = Window.partitionBy('userId').orderBy(F.desc('ts'))\n",
    "\n",
    "# add next_date column \n",
    "df_delta_login = df_clean.withColumn('next_date',lag('date',1).over(windowval))\n",
    "\n",
    "# calculate the delta between two login \n",
    "df_delta_login = df_delta_login.withColumn(\"delta_time\", F.datediff(F.col('next_date'),F.col('date')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the avg delta_time between login for each user \n",
    "\n",
    "delta_cols = ['userId','level','churn_service','churn_paid','delta_time']\n",
    "\n",
    "df_delta_login = df_delta_login.select(delta_cols)\\\n",
    "    .filter(df_delta_login['delta_time'] != 0)\n",
    "distinct_session = df_clean.groupby('userId').agg(F.countDistinct('sessionId').alias('user_num_session'))\n",
    "\n",
    "# join: df_delta_login + distinct_session\n",
    "df_delta_login = df_delta_login.join(distinct_session, on=['userId'], how='inner')\n",
    "\n",
    "# calculate avg delta time , total_avg_delta\n",
    "df_delta_login = df_delta_login.withColumn('avg_delta',F.col('delta_time')/F.col('user_num_session'))\\\n",
    "    .withColumn('total_avg_delta',F.sum('avg_delta').over(Window.partitionBy('userId')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19af1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_login = df_delta_login.groupby(['userId','level','churn_service','churn_paid'])\\\n",
    "    .agg(F.max('total_avg_delta').alias('total_avg_delta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function \n",
    "def set_recent_user_level(df):\n",
    "    \"\"\"\n",
    "        Distinct only recent userId, level \n",
    "        \n",
    "        Input:\n",
    "            df(spark dataframe)\n",
    "    \"\"\"\n",
    "    recent_user_level = df_clean.filter(F.col('row_number')==1)\\\n",
    "    .select(['userId','level']).distinct()\n",
    "    \n",
    "    df_result =  recent_user_level.join(df, on=['userId','level'], how='inner')\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_delta_login = set_recent_user_level(delta_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8edfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_login_no_churn = res_delta_login.filter(filter_no_churn).toPandas()\n",
    "delta_login_churn_paid = res_delta_login.filter(filter_churn_paid).toPandas()\n",
    "delta_login_churn_service = res_delta_login.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_delta_login.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33282737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg \n",
    "print(\"churned service user avg delta time of login:\",delta_login_churn_service.total_avg_delta.mean())\n",
    "print(\"churned paid user avg delta time of login:\",delta_login_churn_paid.total_avg_delta.mean())\n",
    "print(\"no churned user avg delta time of login:\",delta_login_no_churn.total_avg_delta.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbf08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=delta_login_churn_service['total_avg_delta'],\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=delta_login_no_churn['total_avg_delta'],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=delta_login_churn_paid['total_avg_delta'],\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=delta_login_no_churn[delta_login_no_churn['level']=='paid']['total_avg_delta'],\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b436db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=900, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg delta time of login : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33eec7",
   "metadata": {},
   "source": [
    "avg delta time of login 결과 \n",
    "- 서비스, 유료멤버쉽에서 이탈한 유저들의 평균 로그인 하는 간격의 시간차가 더 평균적으로 더 짧은것을 확인 할 수 있다. 자주 로그인 하는 유저들이 이탈한다는것을 확인 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722f7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8a44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "060de974",
   "metadata": {},
   "source": [
    "### Impact of daily behavior\n",
    "- page에 해당하는 다른 여러 액션이 이탈율에 영향을 주는지 분석해본다 \n",
    "- 노래를 듣는지, thumb up 같은 요소 분석\n",
    "- 주간 단위로 나눠서 분석한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf=> is_song, is_thumb_up, is_thumb_down \n",
    "is_song = udf(lambda x: 1 if x == 'NextSong' else 0 , IntegerType())\n",
    "is_thumb_up = udf(lambda x:1 if x =='Thumbs Up' else 0, IntegerType())\n",
    "is_thumb_down  = udf(lambda x:1 if x== 'Thumbs Down' else 0, IntegerType())\n",
    "\n",
    "# add column using udf \n",
    "# daily_action\n",
    "df_daily_action = df_clean.withColumn('is_song',is_song(F.col('page')))\\\n",
    "    .withColumn('is_thumb_up',is_thumb_up(F.col('page')))\\\n",
    "    .withColumn('is_thumb_down',is_thumb_down(F.col('page')))\\\n",
    "    .withColumn('weekofyear',F.weekofyear(F.col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create window per week \n",
    "windowval = Window.partitionBy(['userId','year','weekofyear']).orderBy('ts')\\\n",
    "    .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "# compute total_num_song_week, thumb_up_week\n",
    "df_daily_action = df_daily_action.withColumn('total_song_week',F.sum('is_song').over(windowval))\\\n",
    "    .withColumn('total_thumb_up_week',F.sum('is_thumb_up').over(windowval))\\\n",
    "    .withColumn('total_thumb_down_week', F.sum('is_thumb_down').over(windowval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby \n",
    "\n",
    "daily_cols = ['userId','year','weekofyear','level','churn_service','churn_paid']\n",
    "\n",
    "df_daily_filt = df_daily_action.groupby(daily_cols)\\\n",
    "    .agg(F.max('total_song_week').alias('max_song_week'), \\\n",
    "        F.max('total_thumb_up_week').alias('max_thumb_up_week'),\\\n",
    "        F.max('total_thumb_down_week').alias('max_thumb_down_week'))\n",
    "\n",
    "# calculate avg \n",
    "df_daily_filt = df_daily_filt.withColumn('avg_song_week',F.col('max_song_week')/7)\\\n",
    "    .withColumn('avg_thumb_up_week',F.col('max_thumb_up_week')/7)\\\n",
    "    .withColumn('avg_thumb_down_week',F.col('max_thumb_down_week')/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply segmentation \n",
    "avg_week_no_churn = df_daily_filt.filter(filter_no_churn).distinct().toPandas()\n",
    "avg_week_churn_paid = df_daily_filt.filter(filter_churn_paid).distinct().toPandas()\n",
    "avg_week_churn_service = df_daily_filt.filter(filter_churn_service).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_week_no_churn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215233c9",
   "metadata": {},
   "source": [
    "###### avg_num_song_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=avg_week_churn_service['avg_song_week'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=avg_week_no_churn['avg_song_week'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=avg_week_churn_paid['avg_song_week'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=avg_week_no_churn[avg_week_no_churn['level']=='paid']['avg_song_week'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=900, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg song count per week : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0fee4",
   "metadata": {},
   "source": [
    "avg_song_count per week 결과 \n",
    "- 서비스에서 이탈한 유저가 상대적으로 주간 평균 노래 듣는 횟수가 활성화된 유저보다 적은 편이다.\n",
    "- 유료멤버쉽 에서 이탈한 유저가 상대적으로 활성화된 유료 유저보다 주간 평균 노래 듣ㄷ는 회수가 근소하게 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c624c7",
   "metadata": {},
   "source": [
    "###### avg_thump_up_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=avg_week_churn_service['avg_thumb_up_week'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=avg_week_no_churn['avg_thumb_up_week'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=avg_week_churn_paid['avg_thumb_up_week'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=avg_week_no_churn[avg_week_no_churn['level']=='paid']['avg_thumb_up_week'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69889dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=900, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg thumb up count per week : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc812eba",
   "metadata": {},
   "source": [
    "avg thumb up week 결과\n",
    "- 서비스 이탈 유저가 평균적으로 따봉주는 횟수가 적다 \n",
    "- 유료 멤버쉽 이탈 유저가 평균적으로 따봉주는 횟수가 많다 \n",
    "\n",
    "- 결과적으로 서비스 이탈 유저는 노래를 듣거나, 따봉을 주거나 하는 daily action 에서의 빈도가 낮고, 유료 멤버쉽 이탈 유저는 daily action 에서의 빈도가 잦다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38af372",
   "metadata": {},
   "source": [
    "#### Impact of the listening time per session\n",
    "- 세션마다 노래를 듣는 시간을 분석해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window parititon by userId and sorting by descending timestamp\n",
    "window_user_ts = Window.partitionBy('userId').orderBy(desc('ts'))\n",
    "\n",
    "# session window \n",
    "window_session = Window.partitionBy(['userId','sessionId']).orderBy('ts')\\\n",
    "    .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "# new column next_ts, next_action\n",
    "\n",
    "df_listen_session = df_clean.withColumn('next_ts',lag('ts',1).over(window_user_ts))\\\n",
    "    .withColumn('next_action',lag('page',1).over(window_user_ts))\\\n",
    "    .filter(F.col('page')=='NextSong')\n",
    "\n",
    "# calculate the difference between two timestamp\n",
    "df_listen_session = df_listen_session.withColumn('diff_ts',(F.col('next_ts')-F.col('ts'))/1000)\n",
    "\n",
    "# add column with total listening per session \n",
    "# 세션별로 노래 들은 시간 ts => list_session\n",
    "df_listen_session = df_listen_session.withColumn('listen_session',F.sum('diff_ts').over(window_session))\n",
    "\n",
    "# 즉 list_Session의 총합 max로 해서 \n",
    "# 그룹으로 묶어버리기 \n",
    "\n",
    "df_listen_session_filt = df_listen_session.groupby(['userId','sessionId','level','churn_service','churn_paid'])\\\n",
    "    .agg(F.max('listen_session').alias('total_listen_session'),\\\n",
    "        F.max('iteminSession').alias('item_session'))\n",
    "\n",
    "# avg listen time session \n",
    "df_listen_session_filt = df_listen_session_filt.withColumn('avg_listen_session',\n",
    "                                                        F.round(F.col('total_listen_session')/F.col('item_session')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation \n",
    "\n",
    "listen_session_no_churn = df_listen_session_filt.filter(filter_no_churn).toPandas()\n",
    "listen_session_churn_paid= df_listen_session_filt.filter(filter_churn_paid).toPandas()\n",
    "listen_session_churn_service = df_listen_session_filt.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d805da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "listen_session_no_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=listen_session_churn_service['avg_listen_session'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=listen_session_no_churn['avg_listen_session'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=listen_session_churn_paid['avg_listen_session'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=listen_session_no_churn[listen_session_no_churn['level']=='paid']['avg_listen_session'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d36736",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=900, width=1200, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg listen time per session : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647757c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e897842",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=['avg_listen_time','avg_item_in_session'],\n",
    "    y=[listen_session_churn_service.avg_listen_session.mean(),listen_session_churn_service.item_session.mean()],\n",
    "    name='churn service user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=['avg_listen_time','avg_item_in_session'],\n",
    "    y=[listen_session_no_churn.avg_listen_session.mean(),listen_session_no_churn.item_session.mean()],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=['avg_listen_time','avg_item_in_session'],\n",
    "    y=[listen_session_churn_paid.avg_listen_session.mean(),listen_session_churn_paid.item_session.mean()],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    x=['avg_listen_time','avg_item_in_session'],\n",
    "    y=[listen_session_no_churn[listen_session_no_churn['level']=='paid'].avg_listen_session.mean(),\n",
    "      listen_session_no_churn[listen_session_no_churn['level']=='paid'].item_session.mean()],\n",
    "    name='active paid user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "\n",
    "fig.layout.update(height=600, width=900, )\n",
    "# fig.update_xaxes(\n",
    "#     title_text = 'avg listen time'\n",
    "# )\n",
    "fig.update_yaxes(\n",
    "    title_text = 'avg listen time per session'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41cd9cd",
   "metadata": {},
   "source": [
    "avg_listen_time per session+ avg_number_item per session 결과\n",
    "- 서비스 이탈 유저보다 현재 활성화 되어 있는 유저가 세션마다 듣는 시간이 평균적으로 높다 \n",
    "- 서비스 이탈 유저보다 활성화 유저의 세션별 아이템 수가 평균적으로 높다\n",
    "- 유료 멤버쉽 이탈 유저의 평균 세션별 듣는 시간이 현재 유료 활성화 유저보다 높다 \n",
    "- 유료 멤버쉽 이탈 유저의 평균 세션별 아이템 수가 현재 유료 활성화 유저보다 낮으므로, \n",
    "    유료 멤버쉽 이탈 유저는 아무튼 세션별 평균적으로 노래를 오래 듣는다고 파악할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b097b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538ad06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041ec20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7cd29ea",
   "metadata": {},
   "source": [
    "#### Impact of the time of the activity (count of actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d6893",
   "metadata": {},
   "source": [
    "- 각 유저의 행동 횟수를 page 컬럼을 통해 구해본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8169d",
   "metadata": {},
   "source": [
    "###### avg_action_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ad9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of actions per user \n",
    "df_action_user = df_clean.groupby(['userId','level','churn_service','churn_paid'])\\\n",
    "    .agg(F.count('page').alias('action_per_user')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of actions per day \n",
    "df_action_user_day = df_clean.groupby(['userId','year','month','day'])\\\n",
    "    .agg(F.count('page').alias('action_per_day')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_action_day = pd.merge(df_action_user, df_action_user_day,on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bac0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_action_day['avg_action_per_day'] = avg_action_day['action_per_day']/avg_action_day['action_per_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_action_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set segmentation \n",
    "\n",
    "where_churn_paid = (avg_action_day['churn_paid']!=0)\n",
    "where_churn_service = (avg_action_day['churn_service']!=0)\n",
    "where_no_churn = (~where_churn_paid)&(~where_churn_service)\n",
    "\n",
    "action_day_no_churn = avg_action_day[(avg_action_day['churn_service']==0)&(avg_action_day['churn_paid']==0)]\n",
    "action_day_churn_paid = avg_action_day[(avg_action_day['churn_paid']!=0)&(avg_action_day['churn_service']==0)]\n",
    "action_day_churn_service = avg_action_day[avg_action_day['churn_service']!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda36b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data quality count check \n",
    "assert avg_action_day.shape[0] == (action_day_no_churn.shape[0]+action_day_churn_paid.shape[0]+action_day_churn_service.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b581f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot unique \n",
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=action_day_churn_service['avg_action_per_day'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=action_day_no_churn['avg_action_per_day'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=action_day_churn_paid['avg_action_per_day'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=action_day_no_churn[action_day_no_churn['level']=='paid']['avg_action_per_day'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=700, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg count of action per day : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e60b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad6d94c",
   "metadata": {},
   "source": [
    "##### avg_action per week, weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf626fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_action_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78458457",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_weekday = udf(lambda x:1 if x in [1,2,3,4,5] else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_action = df_clean.withColumn('is_weekday',is_weekday('dayofweek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_action_weekday = df_week_action.filter(F.col('is_weekday')==1)\\\n",
    "    .groupby(['userId','level'])\\\n",
    "    .agg(F.count('page').alias('count_action_weekday'))\n",
    "df_week_action_weekend = df_week_action.filter(F.col('is_weekday')!=1)\\\n",
    "    .groupby(['userId','level'])\\\n",
    "    .agg(F.count('page').alias('count_action_weekend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_week_action=df_week_action.groupby(['userId','level','churn_service','churn_paid','is_weekday'])\\\n",
    "    .agg(F.count('page').alias('count_action_week'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set segmentation \n",
    "filter_weekday = F.col('is_weekday') == 1\n",
    "filter_weekend = ~(filter_weekday)\n",
    "# where_weekday = \n",
    "count_action_weekday = count_week_action.filter(filter_weekday)\n",
    "count_action_weekend = count_week_action.filter(filter_weekend)\n",
    "\n",
    "\n",
    "action_weekday_no_churn = count_action_weekday.filter(filter_no_churn).toPandas()\n",
    "action_weekday_churn_paid = count_action_weekday.filter(filter_churn_paid).toPandas()\n",
    "action_weekday_churn_service = count_action_weekday.filter(filter_churn_service).toPandas()\n",
    "\n",
    "action_weekend_no_churn = count_action_weekend.filter(filter_no_churn).toPandas()\n",
    "action_weekend_churn_paid = count_action_weekend.filter(filter_churn_paid).toPandas()\n",
    "action_weekend_churn_service = count_action_weekend.filter(filter_churn_service).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=['weekday','weekend'],\n",
    "    y=[action_weekday_churn_service.count_action_week.mean(),\n",
    "       action_weekend_churn_service.count_action_week.mean()],\n",
    "    name='churn service user'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=['weekday','weekend'],\n",
    "    y=[action_weekday_no_churn.count_action_week.mean(),\n",
    "       action_weekend_no_churn.count_action_week.mean()],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=['weekday','weekend'],\n",
    "    y=[action_weekday_churn_paid.count_action_week.mean(),\n",
    "       action_weekend_churn_paid.count_action_week.mean()],\n",
    "    name='churn paid user'\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    x=['weekday','weekend'],\n",
    "    y=[action_weekday_no_churn[action_weekday_no_churn['level']=='paid'].count_action_week.mean(),\n",
    "      action_weekend_no_churn[action_weekend_no_churn['level']=='paid'].count_action_week.mean()],\n",
    "    name='active paid user'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "\n",
    "fig.layout.update(height=600, width=900, )\n",
    "# fig.update_xaxes(\n",
    "#     title_text = 'avg listen time'\n",
    "# )\n",
    "fig.update_yaxes(\n",
    "    title_text = 'avg count of action per weekday of weekend'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95512d0a",
   "metadata": {},
   "source": [
    "###### 주말 vs 평일 비교 결과 \n",
    "- 전체적으로 평일에 이용을 많이 한다 \n",
    "- 서비스 ,유료 멤버쉽 이탈 유저들이 평균적으로 평일이나 주말이나 action의 횟수가 적다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set segmentation \n",
    "# filter_weekday = F.col('is_weekday') == 1\n",
    "# filter_weekend = ~(filter_weekday)\n",
    "\n",
    "\n",
    "action_week_no_churn = count_week_action.filter(filter_no_churn).toPandas()\n",
    "action_week_churn_paid = count_week_action.filter(filter_churn_paid).toPandas()\n",
    "action_week_churn_service = count_week_action.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot unique \n",
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=action_week_churn_service['count_action_week'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=action_week_no_churn['count_action_week'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=action_week_churn_paid['count_action_week'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=action_week_no_churn[action_week_no_churn['level']=='paid']['count_action_week'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=700, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'avg count of action on weekday : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5056e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fa682a1",
   "metadata": {},
   "source": [
    "##### impact of the time between registration, upgrade and downgrade events\n",
    "- 회원 가입 시간부터 최근 활동 시간까지의 기간 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ts on registration \n",
    "\n",
    "regi_ts = (F.col('registration')/1000).cast('timestamp')\n",
    "\n",
    "df_regi = df_clean.withColumn('regi_date',F.date_format(regi_ts,format='yyyy-MM-dd'))\\\n",
    "    .withColumn('regi_date',F.to_date(F.col('regi_date'),'yyyy-MM-dd'))\\\n",
    "        .withColumn('regi_year',F.year(F.col('date')))\\\n",
    "        .withColumn('regi_month',F.month(F.col('date')))\\\n",
    "        .withColumn('regi_day',F.dayofmonth(F.col('date')))\\\n",
    "        .withColumn('regi_hour', F.hour(regi_ts))\\\n",
    "        .withColumn('regi_dayofweek',F.dayofweek(F.col('date')))\\\n",
    "        .withColumn('regi_weekofyear',F.weekofyear(F.col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c266d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last_interaction , days_from_reg colu,n\n",
    "df_from_reg = df_regi.filter(F.col('row_number')==1)\\\n",
    "    .groupby(['userId','level','churn_service','churn_paid','regi_date'])\\\n",
    "    .agg(F.max('date').alias('last_interaction'))\\\n",
    "    .withColumn('day_from_reg',F.datediff('last_interaction','regi_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd57a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_reg.orderBy('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot \n",
    "\n",
    "from_reg_no_churn = df_from_reg.filter(filter_no_churn).toPandas()\n",
    "from_reg_churn_paid = df_from_reg.filter(filter_churn_paid).toPandas()\n",
    "from_reg_churn_service = df_from_reg.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg \n",
    "print(\"Avg of day_from_reg: Active vs churned from service\\n\",\\\n",
    "     from_reg_no_churn['day_from_reg'].mean(), from_reg_churn_service['day_from_reg'].mean())\n",
    "print(\"Avg of day_from_reg: Active paid vs churned from paid\\n\",\\\n",
    "     from_reg_no_churn[from_reg_no_churn['level']=='paid']['day_from_reg'].mean(), from_reg_churn_paid['day_from_reg'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot unique \n",
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=from_reg_churn_service['day_from_reg'].unique(),\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=from_reg_no_churn['day_from_reg'].unique(),\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=from_reg_churn_paid['day_from_reg'].unique(),\n",
    "    name='churn paid ser'\n",
    ")\n",
    "trace4 = go.Box(\n",
    "    x=from_reg_churn_paid[from_reg_churn_paid['level']=='paid']['day_from_reg'].unique(),\n",
    "    name='active paid user'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f414007",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Service cancellation analysis','Service downgrade analysis']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,2,1)\n",
    "fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=700, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'distribution of days from registration : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7e756",
   "metadata": {},
   "source": [
    "day_from_reg 분석 결과 \n",
    "- 서비스 이탈 유저는 가입한 시간으로부터 최근 활동시간까지의 기간이 짧다\n",
    "- 유료 멤버십 이탈 유저와 활성화된 유료 유저와의 큰 차이는 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be331949",
   "metadata": {},
   "source": [
    "##### upgrade and downgrade\n",
    "- from upgrade ~ downgrade_date\n",
    "- from reg ~ to upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75813ce1",
   "metadata": {},
   "source": [
    "#### from_reg ~ to upgrade\n",
    "- 회원등록부터 유료결제 까지의 걸리는 평균 시간을 분석해본다\n",
    "- 유료결제에서 유료 해지까지의 걸리는 평균 시간을 분석해본다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461ebcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "### upgrade_df \n",
    "\n",
    "df_upgrade = df_regi.select(['userId','level','page','upgraded','phase_upgrade','churn_paid','churn_service','ts','date','regi_date'])\\\n",
    "    .filter(F.col('page')=='Submit Upgrade')\\\n",
    "    .filter(F.col('level')=='free').dropDuplicates()\\\n",
    "    .withColumn('reg_to_upgrade',F.datediff('date','regi_date'))\\\n",
    "    .withColumn('upgrade_date',F.col('date'))\\\n",
    "    .orderBy('userId',F.desc('phase_upgrade'),F.asc('date'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowval = Window.partitionBy('userId').orderBy(F.asc('ts'))\n",
    "upgrade_user = df_upgrade.withColumn('row_number',F.row_number().over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row number == 1\n",
    "upgrade_user = upgrade_user.filter(F.col('row_number')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade segmentation \n",
    "upgrade_no_churn = upgrade_user.filter(filter_no_churn).toPandas()\n",
    "upgrade_churn_paid = upgrade_user.filter(filter_churn_paid).toPandas()\n",
    "upgrade_churn_service = upgrade_user.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d52ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot unique \n",
    "# 1.churn_service vs active\n",
    "# churn service\n",
    "trace1 = go.Box(\n",
    "    x=upgrade_churn_service['reg_to_upgrade'],\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=upgrade_no_churn['reg_to_upgrade'],\n",
    "    name='active user'\n",
    ")\n",
    "\n",
    "# 2. churn paid: churn_paid vs active paid\n",
    "# churn_service_free_user\n",
    "trace3 = go.Box(\n",
    "    x=upgrade_churn_paid['reg_to_upgrade'],\n",
    "    name='churn paid ser'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1470bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    subplot_titles=['Distribution upgrade time from registration']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,1)\n",
    "# fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=700, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'distribution of days to upgrade from registration : Active vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da44ebc",
   "metadata": {},
   "source": [
    "from_registration_to_upgrade 결과:\n",
    "- 서비스에서 이탈한 유저는 유료결제 멤버쉽에도 빠르게 반응 하는 경향이 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cecb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upgrade.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create upgrade_downgrade dataframe\n",
    "up_down_cols = ['userId','level','page','phase_upgrade','phase_downgrade','churn_paid','churn_service','date','ts']\n",
    "\n",
    "df_up_down=df_regi.select(up_down_cols)\\\n",
    "    .filter((F.col('page')=='Submit Upgrade')|(F.col('page')=='Submit Downgrade'))\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44596d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create next_date , next_action, up_to_downgrade column\n",
    "windowval = Window.partitionBy('userId').orderBy(F.desc('ts'))\n",
    "\n",
    "df_up_down = df_up_down.withColumn('next_date',lag('date',1).over(windowval))\\\n",
    "    .withColumn('next_action',lag('page',1).over(windowval))\\\n",
    "    .withColumn('up_to_downgrade',F.datediff('next_date','date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae22c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer segmentation \n",
    "\n",
    "# 1. churn_Service !=0\n",
    "up_down_churn_paid = df_up_down.filter(filter_churn_paid).toPandas()\n",
    "up_down_churn_service = df_up_down.filter(filter_churn_service).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890af9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot \n",
    "# box plot unique \n",
    "\n",
    "trace1 = go.Box(\n",
    "    x=up_down_churn_service['up_to_downgrade'],\n",
    "    name='churned service user'\n",
    ")\n",
    "# active user\n",
    "trace2 = go.Box(\n",
    "    x=up_down_churn_paid['up_to_downgrade'],\n",
    "    name='churn paid user'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    subplot_titles=['Distribution downgrade period from upgrade']\n",
    ")\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "# fig.append_trace(trace3,1,1)\n",
    "# fig.append_trace(trace4,2,1)\n",
    "\n",
    "fig.layout.update(height=700, width=700, )\n",
    "fig.update_xaxes(\n",
    "    title_text = 'distribution of days to downgrade from upgrade : churn_paid vs churned user'\n",
    ")\n",
    "# fig.update_yaxes(\n",
    "#     title_text = ''\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a837522",
   "metadata": {},
   "source": [
    "from_upgrade to downgrade 결과\n",
    "- 서비스에서 이탈하고, 유료멤버쉽에서 이탈한 유저가 짧은시간동안 빠르게 이용하고, 빠르게 서비스에서 이탈하는 \n",
    "경향을 확인할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d9cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1820834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790ae37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623102a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_from_df(df, week_day_start, week_day_last):\n",
    "    \"\"\"\n",
    "        Function to extract the rows corresponding to a given week\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # filter by week day_start, week_day_last \n",
    "    filter_week = (F.col('date')>=week_day_start) &(F.col('date')<=week_day_last)\n",
    "    \n",
    "    # is_weekday or not \n",
    "    is_weekday = udf(lambda x:1 if x in ['1','2','3','4','5'] else 0, IntegerType())\n",
    "    is_ads = udf(lambda x:1 if x == \"Roll Advert\" else 0, IntegerType())\n",
    "    \n",
    "    # apply filter_week \n",
    "    df_week = df.filter(filter_week).orderBy('ts',ascending=False)\\\n",
    "        .withColumn('is_weekday',is_weekday('dayofweek'))\\\n",
    "        .withColumn('is_ads',is_ads('page'))\n",
    "    return df_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b12d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b45eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d53b5e9e",
   "metadata": {},
   "source": [
    "##### week summary Class\n",
    "\n",
    "    1. 주간별 노래, 아티스트 , 로긴, 광고 카운트 \n",
    "    2. 주간별, 평일 카운트+ 주말 카운트\n",
    "    3. 주간별 평균 \n",
    "        - song count \n",
    "        - artist distinct count \n",
    "        - login count \n",
    "        - ads count \n",
    "        - song repeat\n",
    "    4. 주간별 delta_login time \n",
    "    5. 주간별 listen_time session\n",
    "    6. 주간별 num_actions_per_session\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833463ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeekSummary:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df_week):\n",
    "        # get df from given weeek\n",
    "        self.df_week = df_week\n",
    "        # get unique userId ,level\n",
    "        self.users = self.get_users()\n",
    "        # get week summary \n",
    "        self.week_summary = self.get_week_summary()\n",
    "    \n",
    "    def get_users(self):\n",
    "        \"\"\"\n",
    "            Extract the list of unique combination of userId, level\n",
    "        \"\"\"\n",
    "        service_users = self.df_week.select(['userId','level']).distinct()\n",
    "        return service_users\n",
    "    \n",
    "    #last_interaction\n",
    "    def get_last_interaction(self):\n",
    "        \"\"\"\n",
    "            get last interaction date from user\n",
    "        \"\"\"\n",
    "        df_last_time = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(F.max('ts').alias('last_interaction'))\n",
    "        \n",
    "        return df_last_time\n",
    "    \n",
    "    \n",
    "    def compute_count_song(self):\n",
    "        \"\"\"\n",
    "            extract the count of songs per user during week or weekend \n",
    "            1. total count of songs \n",
    "            2. total song per weekdays \n",
    "            3. total song per weekend\n",
    "        \"\"\"\n",
    "        song_week = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(count('song').alias('count_song'))\n",
    "        \n",
    "        song_weekday = self.df_week.filter(F.col('is_weekday')==1)\\\n",
    "            .groupby(['userId','level'])\\\n",
    "            .agg(countDistinct('song').alias('count_song_weekday'))\n",
    "    \n",
    "        song_weekend = self.df_week.filter(F.col('is_weekday')!=1)\\\n",
    "            .groupby(['userId','level'])\\\n",
    "            .agg(countDistinct('song').alias('count_song_weekend'))\n",
    "        \n",
    "        return song_week, song_weekday, song_weekend\n",
    "    \n",
    "    def compute_count_artist(self):\n",
    "        \"\"\"\n",
    "            extract the count of distinct artist listend to per user\n",
    "        \"\"\"\n",
    "        artist_week = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(countDistinct('artist').alias('count_distinct_artist'))\n",
    "        \n",
    "        return artist_week\n",
    "    \n",
    "    def compute_count_login(self):\n",
    "        \"\"\"\n",
    "            Extract the number of session per user\n",
    "        \"\"\"\n",
    "        login_week = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(countDistinct('sessionId').alias('count_login'))\n",
    "        \n",
    "        \n",
    "        return login_week\n",
    "    \n",
    "    def compute_count_ads(self):\n",
    "        \"\"\"\n",
    "            Extract the number of ads listened to per user\n",
    "        \"\"\"\n",
    "        ads_week = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(F.sum('is_ads').alias('count_ads'))\n",
    "        \n",
    "        return ads_week\n",
    "    \n",
    "    #get_last_interaction\n",
    "    #compute_counts\n",
    "    def compute_count_merge(self):\n",
    "        \"\"\"\n",
    "            add column per count\n",
    "        \"\"\"\n",
    "        df_count = self.users\n",
    "        \n",
    "        # add songs on weekdays and weekend \n",
    "        song_week, song_weekday, song_weekend = self.compute_count_song()\n",
    "        \n",
    "        df_count = df_count.join(song_week, on=['userId','level'], how='full')\\\n",
    "            .join(song_weekday, on=['userId','level'], how='full')\\\n",
    "            .join(song_weekend, on=['userId','level'], how='full')\n",
    "        \n",
    "        # add distinct artists \n",
    "        artist_week = self.compute_count_artist()\n",
    "        df_count = df_count.join(artist_week, on=['userId','level'], how='full')\n",
    "        \n",
    "        # add login (unique sessionId?)\n",
    "        login_week = self.compute_count_login()\n",
    "        df_count = df_count.join(login_week, on=['userId','level'], how='full')\n",
    "        \n",
    "        # add ads\n",
    "        ads_week = self.compute_count_ads()\n",
    "        df_count = df_count.join(ads_week, on=['userId','level'], how='full')\n",
    "        \n",
    "        # repeat\n",
    "        df_count = df_count.withColumn('count_repeat',F.col('count_song')-(F.col('count_song_weekday')+F.col('count_song_weekend')))\n",
    "        \n",
    "        return df_count\n",
    "        \n",
    "    def compute_delta_login(self):\n",
    "        \"\"\"\n",
    "            Calculate avg delta time between two login in the given week\n",
    "        \"\"\"\n",
    "        \n",
    "        # set inwdow \n",
    "        window_login = Window.partitionBy(['userId']).orderBy(F.desc('ts'))\n",
    "        \n",
    "        # add new column next date \n",
    "        df_delta = self.df_week.withColumn('next_date',lag('date',1).over(window_login))\n",
    "        \n",
    "        # calculate delta betwwen two login\n",
    "        df_delta = df_delta.withColumn('delta_time',F.datediff(F.col('next_date'),F.col('date')))\n",
    "        \n",
    "        # compute the avg delta_time between login for each user \n",
    "\n",
    "        delta_cols = ['userId','level','churn_service','churn_paid','delta_time']\n",
    "\n",
    "        df_delta_login = df_delta.select(delta_cols)\\\n",
    "            .filter(df_delta_login['delta_time'] != 0)\n",
    "        distinct_session = df_churn.groupby('userId').agg(F.countDistinct('sessionId').alias('user_num_session'))\n",
    "\n",
    "        # join: df_delta_login + distinct_session\n",
    "        df_delta_login = df_delta_login.join(distinct_session, on=['userId'], how='inner')\n",
    "\n",
    "        # calculate avg delta time , total_avg_delta\n",
    "        df_delta_login = df_delta_login.withColumn('avg_delta',F.col('delta_time')/F.col('user_num_session'))\\\n",
    "            .withColumn('total_avg_delta',F.sum('avg_delta').over(Window.partitionBy('userId')))\n",
    "        \n",
    "        # group by , relevant data \n",
    "        df_delta_login = df_delta_login.groupby(['userId','level'])\\\n",
    "            .agg(F.max('total_avg_delta').alias('time_inter_login'))\n",
    "        \n",
    "        \n",
    "        return df_delta_login\n",
    "    \n",
    "    def compute_listen_session(self):\n",
    "        \"\"\"\n",
    "            Calculate the average listening per session per user \n",
    "        \"\"\"\n",
    "        # window per user by desc timestmp\n",
    "        window_user = Window.partitionBy(\"userId\").orderBy(F.desc('ts'))\n",
    "        #window per user, session\n",
    "        window_session = Window.partitionBy([\"userId\",\"sessionId\"]).orderBy(\"ts\").rangeBetween(Window.unboundedPreceding,0)\n",
    "        \n",
    "        # add two new columns: next_ts, next_action\n",
    "        df_listen_session = self.df_week.withColumn('next_ts',lag('ts',1).over(window_user))\\\n",
    "            .withColumn('next_action',lag('page',1).over(window_user))\n",
    "        \n",
    "        # calculate the diff between two timestamp\n",
    "        df_listen_session = df_listen_session.withColumn(\"diff_ts\",(F.col('next_ts').cast('integer')- F.col('ts').cast('integer'))/1000)\n",
    "        \n",
    "        # keep only the Nextsong action , filter \n",
    "        df_listen_session = df_listen_session.filter(F.col['page']=='NextSong')\n",
    "        # add a column total listening \n",
    "        df_listen_session = df_listen_session.withColumn(\"listen_session\",F.sum(\"diff_ts\").over(window_session))\n",
    "        \n",
    "        # extract max value only for each session per user\n",
    "        df_listen_session = df_listen_session.groupby(['userId','sessionId','level'])\\\n",
    "            .agg(F.max('listen_session').alias('total_listen_session'),\\\n",
    "                F.max('itemInSession').alias('item_session'))\n",
    "        \n",
    "        df_listen_session = df_listen_session.withColumn('avg_listen_session',\n",
    "            F.round((F.col('total_list_session')/F.col('item_session'))/60,2))                                            \n",
    "        \n",
    "        # add a column with total number of session , avg_listen_time per session\n",
    "        num_session = self.df_week.groupby(['userId','level'])\\\n",
    "            .agg(countDistinct('sessionId').alias('num_session'))\n",
    "        \n",
    "        df_listen_session = df_listen_session.join(num_session, on=['userId','level'], how='full')\n",
    "        \n",
    "        df_listen_session = df_listen_session.withColumn(\"week_total_listen\",\n",
    "                            F.sum('avg_listen_session').over(Window.partitionBy('userId')))\\\n",
    "            .withColumn('avg_listen_time_session',F.round((F.col('week_total_listen')/F.col('num_session')),2))\n",
    "        \n",
    "        # keep relevant columns and distinct \n",
    "        df_listen_session = df_listen_session.select(['userId','level','avg_listen_time_session']).distinct()\n",
    "    \n",
    "        return df_listen_session\n",
    "        \n",
    "        \n",
    "    def compute_action_session(self):\n",
    "        \"\"\"\n",
    "            calculate the avg number of action per session per user for the given week\n",
    "        \"\"\"\n",
    "        \n",
    "        window_user = Window.partitionBy(\"userId\").orderBy('ts',ascending=False)\n",
    "        window_session = Window.partitionBy(['userId','sessionId']).orderBy('ts').rangeBetween(Window.unboundedPreceding,0)\n",
    "        \n",
    "        #\n",
    "        df_action_session = df_week.groupby(['userId','level','sessionId'])\\\n",
    "            .agg(F.count('page').alias('action_per_session'))\n",
    "        df_action_session = df_action_session.withColumn('total_action_week',\\\n",
    "                                        F.sum('action_per_session').over(Window.partitionBy('userId')))\n",
    "        \n",
    "        # add column with total number of session in the week \n",
    "        \n",
    "        num_session = df_week.groupby(['userId','level'])\\\n",
    "            .agg(F.countDistinct('sessionId').alias('num_session'))\n",
    "        \n",
    "        df_action_session = df_action_session.join(num_session, on=['userId','level'], how='full')\n",
    "        \n",
    "        df_action_session = df_action_session.withColumn('avg_num_action_session',\\\n",
    "                                        F.round(F.col('total_action_week')/F.col('num_session')),2)\n",
    "        \n",
    "        # keep only the relevant columns \n",
    "        df_action_session = df_action_session.select(['userId','level','avg_num_action_session']).distinct()\n",
    "        \n",
    "        return df_action_session\n",
    "    \n",
    "    \n",
    "    def compute_avg(self):\n",
    "        \"\"\"\n",
    "            Add a columns per avg to avg_df one row per user for the given week\n",
    "        \"\"\"\n",
    "        \n",
    "        # only specific user dataframe\n",
    "        df_user_avg = self.users\n",
    "        \n",
    "        # delta time between login\n",
    "        df_delta_login = self.compute_delta_login()\n",
    "        df_user_avg = df_user_avg.join(df_delta_login, on=['userId','level'], how='full')\n",
    "        \n",
    "        # time per session\n",
    "        df_listen_session = self.compute_listen_session()\n",
    "        df_user_avg = df_user_avg.join(df_listen_session, on=['userId','level'], how='full')\n",
    "        \n",
    "        # action per session\n",
    "        df_action_session = self.compute_action_session()\n",
    "        df_user_avg = df_user_avg.join(df_action_session, on=['userId','level'], how='full')\n",
    "        \n",
    "        return df_user_avg\n",
    "    \n",
    "    #replace_na\n",
    "    def replace_na(self, df):\n",
    "        \"\"\"\n",
    "            Sets default values for null values when calculating count and average\n",
    "        \"\"\"\n",
    "        # replace null value of time_inter_login to 7 (number of days 7)\n",
    "        df_filled = df.na.fill({'time_inter_login':'7'})\n",
    "        df_filled = df_filled.na.fill(0)\n",
    "        \n",
    "        return df_filled\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_week_summary(self):\n",
    "        \"\"\"\n",
    "            여기서 최종적으로 week_summary준다 \n",
    "            1. last_interaction time\n",
    "            2. count(song, ads,login, artist distcint, song repeat)\n",
    "            3. avg()\n",
    "            4. join(df_avg,on=['userId','level'])\n",
    "            5. after join, replace nan value\n",
    "        \"\"\"\n",
    "        # 최근 활동시간 \n",
    "        week_last_interaction = self.get_last_interaction()\n",
    "        # 여러가지 count \n",
    "        week_summary_count = self.compute_count_merge()\n",
    "        # join\n",
    "        week_summary = week_last_interaction.join(week_summary_count, on=['userId','level'], how='full')\n",
    "        # avg \n",
    "        week_summary_avg = self.compute_avg()\n",
    "        # summary.join(avg)\n",
    "        week_summary = week_summary.join(week_summary_avg, on=['userId','level'], how='full')\n",
    "        # replace nan value \n",
    "        week_summary = self.replace_na(week_summary)\n",
    "        \n",
    "        return week_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171cada",
   "metadata": {},
   "source": [
    "#### UserSummary:\n",
    "    - 매주 새롭게 이벤트 데이터가 들어오므로 \n",
    "        => 기존의 summary에서 update summary로 업데이트 하는 \n",
    "    - \n",
    "    - \n",
    "    - \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserSummary:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, table_name):\n",
    "        self.df = df \n",
    "        self.table_name = table_name\n",
    "        self.user_summary = None\n",
    "    \n",
    "    def create_column_with_type(self,column_list, target_type):\n",
    "        \"\"\"\n",
    "            Convert Type on column_list to target_type\n",
    "        \"\"\"\n",
    "        for column in column_list:\n",
    "            df = self.df.withColumn(column,lit(None))\\\n",
    "                .withColumn(column, F.col(column).cast(target_type))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def load_summary(self):\n",
    "        \"\"\"\n",
    "            Load existing summary. if there is no existing table, create empty dataframe with specified column type.\n",
    "        \"\"\"\n",
    "         try:\n",
    "            loaded_summary = spark.read.parquet('./spark-warehouse/{}'.format(self.table_name))\n",
    "            return loaded_summary\n",
    "        \n",
    "        except:\n",
    "            # create an empty dataframe \n",
    "            # 1. using StructField \n",
    "            user_summary = spark.createDataFrame([],StructType([]))\n",
    "            \n",
    "            str_cols = ['userId','level','gender','state','last_interaction']\n",
    "            int_cols = ['churn_service','churn_paid','count_song','count_song_weekday','count_song_weekend','count_distinct_artist','count_login','count_ads','count_repeat','day_from_reg']\n",
    "            float_cols = ['time_inter_login','avg_listen_time_session','avg_num_action_session']\n",
    "            \n",
    "            # apply create_column_type\n",
    "            user_summary = create_column_with_type(user_summary,str_cols,StringType())\n",
    "            user_summary = create_column_with_type(user_summary,int_cols,IntegerType())\n",
    "            user_summary = create_column_with_type(user_summary,float_cols,FloatType())\n",
    "            # 2. columns = []\n",
    "            # schema= StructType(columns)\n",
    "            #. user_summary = spark.createDataFrame(data=[],schema=schema)\n",
    "            # convert_type, integeer_cols, string_cols, float_cols\n",
    "            \n",
    "            \n",
    "            return user_summary\n",
    "    \n",
    "    def save_summary(self,mode=\"\"):\n",
    "        \"\"\"\n",
    "            Save summary as data warehouse\n",
    "        \"\"\"\n",
    "        self.user_summary.write\\\n",
    "            .mode(mode)\\\n",
    "            .parquet(\"./spark-warehouse/{}\".format(self.table_name))\n",
    "        \n",
    "        print(\"Saved summary!\")\n",
    "        \n",
    "    \n",
    "    # update 1.day_from_reg\n",
    "    def compute_day_from_reg(self, updated_summary):\n",
    "        \"\"\"\n",
    "            Computes day from reg using last_interaction\n",
    "        \"\"\"\n",
    "        regi_summary = self.df.select(['userId','level','regi_date']).distinct()\n",
    "        \n",
    "        #join input_df + reg_summary\n",
    "        updated_summary = updated_summary.join(regi_summary,on=['userId','level'])\n",
    "        \n",
    "        # calculate day_from_reg, with F.datediff().\n",
    "        updated_summary = updated_summary.withColumn('day_from_reg',\\\n",
    "                                                    F.datediff('last_interaction','regi_date'))\n",
    "        \n",
    "        # drop regi_date\n",
    "        updated_summary = updated_summary.drop('regi_date')\n",
    "        \n",
    "        return updated_summary \n",
    "    \n",
    "    \n",
    "    # init_user_summary \n",
    "    def init_user_summary(self, updated_summary):\n",
    "        \"\"\"\n",
    "            create 'day_from_registration' column using compute_day_from_reg function\n",
    "        \"\"\"\n",
    "        updated_summary = self.compute_day_from_reg(updated_summary)\n",
    "        \n",
    "        return updated_summary\n",
    "    \n",
    "    def get_last_summary(self):\n",
    "        \"\"\"\n",
    "            return last version of the summary(version saved) and rename the columns\n",
    "        \"\"\"\n",
    "        # rename으로 구분\n",
    "        \n",
    "        user_summary = self.user_summary.createOrReplaceTempView(\"user_summary\")\n",
    "        user_summary = spark.sql(\"SELECT * FROM user_summary\")\n",
    "        \n",
    "        last_user_summary = spark.sql(\"\"\"\n",
    "            SELECT * FROM (\n",
    "                SELECT *, MAX(last_interaction) OVER (PARTITION BY userId,level) AS max_last FROM user_summary\n",
    "            ) table1 \\\n",
    "            WHERE last_interaction = table1.max_last\n",
    "        \"\"\")\n",
    "        \n",
    "        # rename count_cols => last_count_cols \n",
    "        count_cols =['count_song','count_song_weekday','count_song_weekend',\\\n",
    "                    'count_distinct_artist','count_login','count_ads','count_repeat',\\\n",
    "                    'time_inter_login','avg_listen_time_session','avg_num_action_session']\n",
    "        # for loop\n",
    "        # apply rename column\n",
    "        for col in count_cols:\n",
    "            last_user_summary = last_user_summary.withColumnRenamed(col, 'last_{}'.format(col))\n",
    "            \n",
    "        return last_user_summary\n",
    "    \n",
    "    # update old summary \n",
    "    def update_old_summary(self, updated_summary):\n",
    "        \"\"\"\n",
    "            Appends new rows for each user for the next week, to prev loaded summary.\n",
    "            \n",
    "        \"\"\"\n",
    "        last_user_summary = self.get_last_summary()\n",
    "        \n",
    "        # drop last cols\n",
    "        drop_old_cols = ['last_interaction','gender','state','churn_service','churn_paid','max_last']\n",
    "        last_user_summary = last_user_summary.drop(drop_old_cols)\n",
    "        \n",
    "        # split the user_summary into 2df: one with knownuser, one with new user \n",
    "        # for new user, apply instantiation of the summary \n",
    "        new_user_summary = updated_summary.join(last_user_summary, on=['userId','level'],\\\n",
    "                                               how='left_anti')\n",
    "        \n",
    "        new_user_summary = new_user_summary.select(['userId','level','last_interaction',\\\n",
    "                                                   'count_song','count_song_weekday','count_song_weekend',\\\n",
    "                                                   'count_distinct_artist','count_login','count_ads','count_repeat',\\\n",
    "                                                   'time_inter_login','avg_listen_time_session','avg_num_action_session',\\\n",
    "                                                   'gender','state','churn_service','churn_paid'])\n",
    "        \n",
    "        # create new user summary  with calculatting day_from_reg\n",
    "        new_user_summary = self.init_user_summary(user_summary_new)\n",
    "        \n",
    "        # for old user too, day_from_reg\n",
    "        old_user_summary = updated_summary.join(last_user_summary, on=['userId','level'],how='inner')\n",
    "        \n",
    "        # drop column last_{} count column \n",
    "        count_cols = ['count_song', 'count_song_weekday', 'count_song_weekend',\\\n",
    "                     'count_distinct_artist', 'count_login', 'count_ads', 'count_repeat',\\\n",
    "                     'time_inter_login', 'avg_listen_time_session', 'avg_num_action_session']\n",
    "        \n",
    "        for column in count_cols:\n",
    "            last_count_col='last_{}'.format(column)\n",
    "            old_user_summary = old_user_summary.drop(last_count_col)\n",
    "        \n",
    "        # add day_from_reg column \n",
    "        old_user_summary = self.compute_time_from_reg(old_user_summary)\n",
    "        \n",
    "        # union new + old\n",
    "        updated_summary = old_user_summary.union(new_user_summary)\n",
    "        \n",
    "        return udpated_summary\n",
    "      \n",
    "    # final update\n",
    "    def update_user_summary(self, week_sum):\n",
    "        \"\"\"\n",
    "            update user summary, compute again day from registration and last_interaction\n",
    "        \"\"\"\n",
    "        week_sum = week_sum.createOrReplaceTempView(\"week_sum\")\n",
    "        week_sum = spark.sql(\"SELECT * FROM week_sum\")\n",
    "        \n",
    "        # instantiate the update\n",
    "        user_info = self.df.select(['userId','level','gender','state','churn_service','churn_paid']).distinct()\n",
    "        updated_summary = week_sum.join(user_info, on=['userId','level'], how='inner')\n",
    "        \n",
    "        # load the existing summary \n",
    "        user_summary = self.load_summary()\n",
    "        self.user_summary = user_summary\n",
    "        \n",
    "        if self.user_summary.count() >0:\n",
    "            # compute updated counts \n",
    "            new_user_summary = self.updated_old_summary(updated_summary)\n",
    "            self.user_summary = new_user_summary \n",
    "            \n",
    "            # append save \n",
    "            self.save_summary('append')\n",
    "        else:\n",
    "            # no count, first week?\n",
    "            \n",
    "            updated_summary = self.init_user_summary(updated_summary)\n",
    "            self.user_summary = updated_summary \n",
    "            \n",
    "            self.save_summary('overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175ae15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85441403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549eaf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6921050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027ab64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973ec30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f578b4b4",
   "metadata": {},
   "source": [
    "##### MonthSummary; \n",
    "    -\n",
    "    - \n",
    "    - \n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642d7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
